---
aliases:
---
# תרגיל בית 5

|  | סטודנט א' |
| ---- | ---- |
| **שם** | עידו פנג בנטוב |
| **ת"ז** | 322869140 |
| **דואר אלקטרוני** | ido.fang@campus.technion.ac.il |

|  | סטודנט ב' |
| ---- | ---- |
| **שם** |  ניר קרל |
| **ת"ז**  | 322437203 |
| **דואר אלקטרוני** | nir.karl@campus.technion.ac.il |

## תרגיל 1

### סעיף א'

הקוד הכולל של סעיף א' וסעיף ב' ב-`python`:

```python
import numpy as np
import matplotlib.pyplot as plt

def weight(j, point_x):
    w = 1
    n = len(point_x)
    for i in range(n):
        if i != j:
            w *= (1 / (point_x[j] - point_x[i]))
    return w

def helper_polynom(x, point_x):
    phi = 1
    n = len(point_x)
    for i in range(n):
        phi *= (x - point_x[i])
    return phi

```
```python
    
def lagrange(f, n, a, b):
    point_x = np.linspace(a, b, n)
    point_y = f(point_x)
    w = [weight(j, point_x) for j in range(n)]
    
    # Evaluate p at 0.01 intervals
    t = 0.01
    x = np.linspace(a, b, int((b - a) / t) + 1)
    p = np.zeros_like(x)
    phi = np.zeros_like(x)
    for k in range(len(x)):
        for j in range(n):
            p[k] += (w[j] * point_y[j]) / (x[k] - point_x[j])
        phi[k] = helper_polynom(x[k], point_x)
        p[k] = phi[k] * p[k]
    return x, p, phi

def f(x):
    return np.cos(2 * np.pi * x)

a = -1
b = 1
n = [2, 5, 10, 20]

# n derivative of f

f_n = [
    lambda x: -((2 * np.pi) ** 2) * np.cos(2 * np.pi * x),
    lambda x: -((2 * np.pi) ** 5) * np.sin(2 * np.pi * x),
    lambda x: -((2 * np.pi) ** 10) * np.cos(2 * np.pi * x),
    lambda x: ((2 * np.pi) ** 20) * np.cos(2 * np.pi * x)
]

graph_i = plt.figure()
plt.title("Interpolant evaluations")
plt.plot(np.linspace(a, b, 100), f(np.linspace(a, b, 100)))

```
```python

for k in range(len(n)):
    # Interpolant evaluation
    plt.figure(graph_i.number)
    
    x, p, phi = lagrange(f, n[k], a, b)
    plt.plot(x, p, "--")
    
    # Absolute error
    graph_e = plt.figure()
    plt.title("n = " + str(n[k]) + " error")
    
    fval = f(x)
    e_abs = np.abs(fval - p)
    plt.plot(x, e_abs)
    
    # Error boundaries
    fnxi = np.max(np.abs(f_n[k](np.arange(-1, 1.25, 0.25))))
    e_boundary = np.abs(phi) * (fnxi * (1 / np.math.factorial(n[k])))

    plt.plot(x, e_boundary, "--")
    plt.legend(["Absolute error", "Error boundary"])

plt.figure(graph_i.number)
plt.legend(["f(x)", "n=2", "n=5", "n=10", "n=20"])
plt.show()

```

![[NUM1_HW005/evals.png|book]]

>[!notes] הערה: 
>
 >הגרפים של $f(x)$ ו-$p_{19}$ מאוד דומים, ולכן הם פשוט נמצאים בגרף אחד על השני.
 >

### סעיף ב'

מאחר והנגזרות של $f(x)$ הן כולן פונקציות של $\cos(2\pi x)$ או $\sin(2\pi x)$ בכפולות שונות, ערכן המקסימלי תמיד יתקבל באחד מהערכים:
$$x=-1,-0.75,-0.5,\dots,\, 0.75,\, 1 $$
ולכן מספיק לחשב את ערכי הנגזרת רק בהן. ברור שעבור רוב הנקודות הערכים יהיו זהים, אבל זה רק מוסיף כמה בדיקות בודדות, אז טוב לקחת מקדם ביטחון.

>[!notes] הערה: 
>
 >ה-$n$ שלנו בשאלה הוא שונה מה-$n$ שנתונה בנוסחה (וזה למה הוגדר $N$ שונה מ-$n$ בשאלה). לכן, בקוד, ה-$n$ שאנו משתמשים בו לחישוב הוא $n-1$, כך שמקבלים את הנוסחה:
 >$$\left|e_{n}(x)\right|\leq \dfrac{1}{(n)!}\cdot\psi(x)\cdot \max_{\xi \in [-1,1]}\left|f^{(n)}(\xi)\right|$$
 
 קיבלנו כי החסם לשגיאה אכן מהווה חסם לשגיאה האמיתית:
 
 
 ![[NUM1_HW005/n2.png|book]]
 
![[NUM1_HW005/n5.png|book]]

![[NUM1_HW005/n10.png|book]]

![[NUM1_HW005/n20.png|book]]

<div style="page-break-after: always;"></div>

## תרגיל 2

### סעיף א'

נחשב את פולינום הבסיס עבור כל אחד מה-$x$ הנתונים:
$$L _{i}=\prod_{\begin{array}{c}
j=0 \\
i\neq j
\end{array}}^{2}=\frac{x-x_{j}}{x_{i}-x_{j}}$$

$$\begin{aligned}
&\begin{aligned}
L_{0}(x):& \\
 & j=0:\; i=j\Longrightarrow \emptyset \\
 & j=1:\; \frac{x-0}{-1-0}=-x \\
 & j=2:\; \frac{x-1}{-1-1}=-\frac{x}{2}+\frac{1}{2} \\
&L_{0}(x)=(-x)\cdot \left(- \frac{x}{2}+\frac{1}{2} \right)=\frac{x^{2}}{2}-\frac{x}{2}
\end{aligned}\\
&\begin{aligned}
L_{1}(x):& \\
&j=0:\; \frac{x-(-1)}{0-(-1)}=x+1 \\
&j=1:\;i=j\Longrightarrow \emptyset \\
&j=2:\; \frac{x-1}{0-1}=-x+1 \\
&L _{1(x)}=(x+1)(-x+1)=-x^{2}+1
\end{aligned}\\
&\begin{aligned}\\
L_{2}(x): \\
& j=0:\;\frac{x-(-1)}{1-(-1)}=\frac{x}{2}+\frac{1}{2} \\
& j=1:\;\frac{x-0}{1-0}=x \\
& j=2:\;i=j\Longrightarrow \emptyset \\
&L _{2}=\left( \frac{x}{2}+\frac{1}{2} \right)\cdot x=\frac{x^{2}}{2}+\frac{x}{2}
\end{aligned}\\
\end{aligned}$$

### סעיף ב'
$$\sum_{i=0}^{2}L _{i}(x)=\left( \frac{x^{2}}{2}-\frac{x}{2} \right)+(-x^{2}+1)+\left( \frac{x^{2}}{2}+\frac{x}{2} \right)=1$$
נשים לב שקיבלנו שהסכום של פולינומי הבסיס קבוע, לכן נשער שלא משנה אילו $x$-ים נבחר, נקבל שהסכום של פולינומי הבסיס הוא 1.

נוכיח את ההשערה:
ניקח פונקציה $f(x)=1$.
לפי הגדרה קירוב פולינומי של פולינום הוא הפולינום עצמו, לכן:
$$P(x)=f(x)=\sum_{i=0}^{n}f(x_{i})L_{i}(x)=\sum_{i=0}^{n}L_{i}(x)$$
מכיוון ש-$P(x)$ הוא קירוב פולינומי של $f(x)$  שבחרנו להיות פולינום מסדר $0$ נקבל ש- $P(x)=1=f(x)$
כלומר:
$$P(x)=1=\sum_{i=0}^{n}L_{i}(x)$$
בהוכחה שהצגנו למעלה, לא הייתה תלות ב-$n$ שמייצג את מספר הנקודות שלנו, כלומר את מספר צמדי האינטרפולציה שיהיו לנו. לכן הסכום של פולינומי הבסיס יהיה 1 לא משנה כמה צמדי אינטרפולציה יהיו לנו.

### סעיף ג'

$$P(x)=\sum_{i=0}^{n}y(x_{j})\prod_{\begin{array}{c}
j=0 \\
j\neq i
\end{array}}^{n}\frac{x-x_{j}}{x_{i}-x_{j}}=\sum_{i=0}^{n}y(x_{j})\cdot \prod_{\begin{array}{c}
j=0 \\
j\neq i
\end{array}}^{n}\frac{1}{x_{i}-x_{j}}\cdot \prod_{\begin{array}{c}
j=0 \\
j\neq i
\end{array}}^{n}x-x_{j}$$
$$\prod_{\begin{array}{c}
j=0 \\
j\neq i
\end{array}}^{n}x-x_{j}=(x-x_{1})(x-x_{2})\dots (x-x_{n-1})(x-x_{n})$$
מכיוון שבכל הגורמים יש לנו $x$ ללא מקדם, תוצאת המכפלה של כל הגורמים תכיל ביטוי של $x^{n}$ ללא מקדם - מכיוון שזוהי תוצאת המכפלה של כל ה-$x$ בכל הגורמים.

לכן נקבל:
$$\sum_{i=0}^{n}y(x_{j})\cdot \prod_{\begin{array}{c}
j=0 \\
j\neq i
\end{array}}^{n}\frac{1}{x_{i}-x_{j}}\cdot \prod_{\begin{array}{c}
j=0 \\
j\neq i
\end{array}}^{n}x-x_{j}=\sum_{i=0}^{n}y(x_{j})\cdot \prod_{\begin{array}{c}
j=0 \\
j\neq i
\end{array}}^{n}\frac{1}{x_{i}-x_{j}}\cdot \Big[x^{n}+p(x)\Big]$$
כאשר $p(x)$ זהו פולינום שמייצג את שאר המכפלות שאינן מכילות את $x^{n}$.
מכאן שהמקדם של $x^{n}$ הוא:
$$\sum_{i=0}^{n}y(x_{i})\cdot \prod_{\begin{array}{c}
j=0 \\
j\neq i
\end{array}}^{n}\frac{1}{x_{i}-x_{j}}$$

<div style="page-break-after: always;"></div>

## תרגיל 3
$$u(t)={\gamma}_{1}e^{{\gamma}_{2}t}$$
### סעיף א'
לא נוכל להשתמש בשיטת רגרסיה לינארית כדי לפתור ישירות את הבעיה. הסיבה היא שאחד מהתנאים לשימוש בשיטה זו היא שהפונקציה שאליה אנו רוצים להתאים, $u$, תהיה מהצורה הבאה:
$$u(t)=\begin{pmatrix}
{f}_{1}(t) \\
{f}_{2}(t) \\
\vdots 
\end{pmatrix}\begin{pmatrix}
{\gamma}_{1} \\
{\gamma}_{2} \\
\vdots 
\end{pmatrix}=\bar{f}(t)\bar{\gamma}$$
אבל כל הפרמטרים ${\gamma}_{1},{\gamma}_{2}$ לא לינאריים במשוואה שלנו, ולכן לא נוכל לרשום את $u$ בצורה זו.
כן נוכל לפתור את הבעיה הזאת בעקיפין עם שיטת הרגרסיה, אם נגדיר פונקציה חדשה, $v(t)$ כפי שאנו נעשה בסעיף ב'.


### סעיף ב'
$$\begin{aligned}
y(t)&=\ln (u(t)) \\
&=\ln{\gamma}_{1}+{\gamma}_{2} t
\end{aligned}$$
נסמן:
$${\alpha}_{1}=\ln{\gamma}_{1}$$
נבנה את המערכת משוואות:
$$\begin{aligned}
 & {\alpha}_{1}+{\gamma}_{2}{t}_{1}={y}_{1} \\
 & {\alpha}_{1}+{\gamma}_{2}{t}_{2}={y}_{2} \\
 &  \quad \quad \,\,\, \vdots  \\
 & {\alpha}_{1}+{\gamma}_{2}t_{n}=y_{n}
\end{aligned}$$
בצורה מטריציונית:
$$\begin{pmatrix}
1 & {t}_{1} \\
1 & {t}_{2} \\
\vdots  & \vdots  \\
1 & t_{n}
\end{pmatrix}\begin{pmatrix}
{\alpha}_{1} \\
{\gamma}_{2}
\end{pmatrix}=\begin{pmatrix}
{y}_{1} \\
{y}_{2} \\
\vdots  \\
y_{n}
\end{pmatrix}$$
נפתור כעת את הבעיה:

$$\begin{gathered}
(A^{T}A)\hat{x}=A^{T}b \\[2ex]
\begin{pmatrix}
1 & 1 & \cdots  & 1 \\
{t}_{1} & {t}_{2} & \cdots  & t_{n}
\end{pmatrix}\begin{pmatrix}
1 & {t}_{1} \\
1 & {t}_{2} \\
\vdots  & \vdots  \\
1 & t_{n}
\end{pmatrix}\begin{pmatrix}
{\alpha}_{1} \\
{\gamma}_{2}
\end{pmatrix}=\begin{pmatrix}
1 & 1 & \cdots  & 1 \\
{t}_{1} & {t}_{2} & \cdots  & t_{n}
\end{pmatrix}\begin{pmatrix}
{y}_{1} \\
{y}_{2} \\
\vdots  \\
y_{n}
\end{pmatrix} \\[2ex]
\begin{pmatrix}
n & \sum_{i=1}^{n}t_{i} \\
\sum_{i=1}^{n}t_{i} & \sum_{n=1}^{n}(t_{i})^{2}  
\end{pmatrix}\begin{pmatrix}
{\alpha}_{1} \\
{\gamma}_{2}
\end{pmatrix}=\begin{pmatrix}
\sum_{i=1}^{n}y_{i} \\
\sum_{i=1}^{n}t_{i}y_{i}  
\end{pmatrix}
\end{gathered}$$


### סעיף ג'
נציב את הנתונים במערכת משוואות:

$$\begin{gathered}
\begin{pmatrix}
n & \sum_{i=1}^{n}t_{i} \\
\sum_{i=1}^{n}t_{i} & \sum_{n=1}^{n}(t_{i})^{2}  
\end{pmatrix}\begin{pmatrix}
{\alpha}_{1} \\
{\gamma}_{2}
\end{pmatrix}=\begin{pmatrix}
\sum_{i=1}^{n}y_{i}\\
\sum_{i=1}^{n}t_{i}y_{i}  
\end{pmatrix} \\[2ex]
\begin{pmatrix}
3 & 3.0 \\
3.0 & 5.0
\end{pmatrix}\begin{pmatrix}
{\alpha}_{1} \\
{\gamma}_{2}
\end{pmatrix}=\begin{pmatrix}
10.9539 \\
17.2378
\end{pmatrix} \\[2ex]
\begin{pmatrix}
3 & 3 \\
0 & 2
\end{pmatrix}\begin{pmatrix}
{\alpha}_{1} \\
{\gamma}_{2}
\end{pmatrix}=\begin{pmatrix}
10.9539 \\
6.2839
\end{pmatrix} \\[2ex]
\begin{pmatrix}
1 & 1 \\
0 & 1
\end{pmatrix}\begin{pmatrix}
{\alpha}_{1} \\
{\gamma}_{2}
\end{pmatrix}=\begin{pmatrix}
3.6513 \\
3.142
\end{pmatrix} \\[2ex]
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}\begin{pmatrix}
{\alpha}_{1} \\
{\gamma}_{2}
\end{pmatrix}=\begin{pmatrix}
0.5093 \\
3.142
\end{pmatrix}
\end{gathered}$$
נסיק כי:
$$\begin{aligned}
 & {\gamma}_{1}=e^{\alpha}=1.6641 \\
 & {\gamma}_{2}=3.142
\end{aligned}$$
ולכן הפונקציה המקורית היא מהצורה:
$$u(t)=1.6641e^{3.142x}$$

הנקודות $(t_{i},y_{i})$, לאחר המרה ל-$u_{i}$:
$$y_{i}=\ln(u_{i})\implies u_{i}=e^{y_{i}}$$
$$\begin{aligned}
 & u_{1}=3.0198 \\
 & u_{2}=11.7001 \\
 & u_{3}=1618.249
\end{aligned}$$

קוד `python` להצגת הגרף והנקודות:
```python
import numpy as np
import matplotlib.pyplot as plt

points_t = [0,1,2]
points_y = [1.1052, 2.4956, 7.3891]

points_u = [np.exp(y) for y in points_y]

plt.plot(points_t, points_u, 'ro')

u = lambda t: 1.6641*np.exp(3.142*t)
ts = np.linspace(0, 2, 100)
plt.plot(ts, [u(t) for t in ts], '-')

plt.title('u(t) = 1.6641*exp(3.142*t)')

plt.yscale('log')
plt.xlabel('t')
plt.ylabel('u')

plt.show()
```

![[NUM1_HW005/output.png|book]]